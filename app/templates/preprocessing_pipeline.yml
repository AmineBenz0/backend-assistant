defaults: &defaults
  template_id: document_preprocessing
  prompt_config_src: backend/libs/llm_service/prompts
  database: default

task: document_preprocessing
<<: *defaults
inputs:
  - client_id
  - project_id
  - filename
  - chunk_size
  - chunk_overlap
  - enable_chunking
  - embedding_model
  - embedding_provider
  - embedding_batch_size

steps:
  - step: get_files
    pipeline_key: GetFiles
    inputs:
      - client_id
      - project_id
    queue: io_queue

  - step: parse_document
    pipeline_key: ParseDocumentToMarkdown
    inputs:
      - get_files
    queue: io_queue

  - step: chunk_document
    pipeline_key: ChunkDocumentForRAG
    inputs:
      - parse_document
      - chunk_size
      - chunk_overlap
      - enable_chunking
    queue: io_queue

  - step: generate_embeddings
    pipeline_key: GenerateChunkEmbeddings
    inputs:
      - chunk_document
      - embedding_model
      - embedding_provider
      - embedding_batch_size
    queue: io_queue

  - step: store_chunks_in_vector_db
    pipeline_key: StoreChunksInVectorDB
    inputs:
      - generate_embeddings
      - client_id
      - project_id
    queue: io_queue

  - step: create_document_mapping
    pipeline_key: CreateDocumentMapping
    inputs:
      - store_chunks_in_vector_db
      - get_files
    queue: io_queue